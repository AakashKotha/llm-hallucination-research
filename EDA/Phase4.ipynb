{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ddf9b7a-e28b-4943-b2fe-9b94cea30b91",
   "metadata": {},
   "source": [
    "### Phase 4: Response Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33e1194b-49e3-4b71-9059-9916d4767dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr, chi2_contingency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d00fcda-cb46-4e13-81ca-fbda2ce6a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "model_names = ['Claude_3.5_Sonnet', 'GPT_3.5', 'GPT_4o']\n",
    "file_paths = [\n",
    "    '../Data/qna_dataset_Claude3.5Sonnet_final.csv',\n",
    "    '../Data/qna_dataset_GPT3.5_final.csv', \n",
    "    '../Data/qna_dataset_GPT4o_final.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceec108f-b62b-45aa-b9c4-decec66b165b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Claude_3.5_Sonnet: 400 rows Ã— 16 columns\n",
      "âœ“ GPT_3.5: 400 rows Ã— 16 columns\n",
      "âœ“ GPT_4o: 400 rows Ã— 16 columns\n"
     ]
    }
   ],
   "source": [
    "for model_name, file_path in zip(model_names, file_paths):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        datasets[model_name] = df\n",
    "        print(f\"âœ“ {model_name}: {df.shape[0]} rows Ã— {df.shape[1]} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecea3f7c-81a3-4101-bfa9-460977797964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert hallucination_present to boolean for all datasets\n",
    "for model_name, df in datasets.items():\n",
    "    if df['hallucination_present'].dtype == 'object':\n",
    "        datasets[model_name]['hallucination_present'] = df['hallucination_present'].map({\n",
    "            'True': True, 'False': False, True: True, False: False\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858ac8b0-750d-4fdd-addd-6b7c564696c1",
   "metadata": {},
   "source": [
    "#### Response length vs accuracy relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2adb12b6-291d-480f-9aa1-b6f467358877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ðŸ“ˆ Claude_3.5_Sonnet:\n",
      "    Binary correlation (length vs hallucination):\n",
      "      Correlation: -0.097\n",
      "      P-value:   0.0536\n",
      "      Significance: (not significant)\n",
      "    Graduated correlation (length vs quality score):\n",
      "      Correlation:  0.001\n",
      "      P-value:   0.9877\n",
      "      Significance: (not significant)\n",
      "    Interpretation: weak negative relationship\n",
      "    ðŸ“ Response length doesn't significantly predict hallucinations\n",
      "\n",
      "  ðŸ“ˆ GPT_3.5:\n",
      "    Binary correlation (length vs hallucination):\n",
      "      Correlation: -0.087\n",
      "      P-value:   0.0821\n",
      "      Significance: (not significant)\n",
      "    Graduated correlation (length vs quality score):\n",
      "      Correlation: -0.081\n",
      "      P-value:   0.1068\n",
      "      Significance: (not significant)\n",
      "    Interpretation: weak negative relationship\n",
      "    ðŸ“ Response length doesn't significantly predict hallucinations\n",
      "\n",
      "  ðŸ“ˆ GPT_4o:\n",
      "    Binary correlation (length vs hallucination):\n",
      "      Correlation: -0.077\n",
      "      P-value:   0.1260\n",
      "      Significance: (not significant)\n",
      "    Graduated correlation (length vs quality score):\n",
      "      Correlation: -0.021\n",
      "      P-value:   0.6714\n",
      "      Significance: (not significant)\n",
      "    Interpretation: weak negative relationship\n",
      "    ðŸ“ Response length doesn't significantly predict hallucinations\n"
     ]
    }
   ],
   "source": [
    "length_accuracy_results = {}\n",
    "    \n",
    "# Define FactScore numeric mapping\n",
    "factscore_numeric = {\n",
    "    'completely right': 4, \n",
    "    'somewhat correct': 3, \n",
    "    'somewhat inaccurate': 2, \n",
    "    'totally wrong': 1\n",
    "}\n",
    "\n",
    "for model_name, df in datasets.items():\n",
    "    print(f\"\\n  ðŸ“ˆ {model_name}:\")\n",
    "    \n",
    "    # Convert factscore to numeric\n",
    "    df_analysis = df.copy()\n",
    "    df_analysis['factscore_numeric'] = df_analysis['factscore'].map(factscore_numeric)\n",
    "    \n",
    "    # Correlation with binary hallucination\n",
    "    binary_corr, binary_p = pearsonr(df_analysis['response_length'], \n",
    "                                    df_analysis['hallucination_present'].astype(int))\n",
    "    \n",
    "    # Correlation with graduated score\n",
    "    # Remove any NaN values for correlation\n",
    "    clean_data = df_analysis.dropna(subset=['response_length', 'factscore_numeric'])\n",
    "    if len(clean_data) > 0:\n",
    "        graduated_corr, graduated_p = pearsonr(clean_data['response_length'], \n",
    "                                              clean_data['factscore_numeric'])\n",
    "    else:\n",
    "        graduated_corr, graduated_p = 0, 1\n",
    "    \n",
    "    print(f\"    Binary correlation (length vs hallucination):\")\n",
    "    print(f\"      Correlation: {binary_corr:6.3f}\")\n",
    "    print(f\"      P-value: {binary_p:8.4f}\")\n",
    "    \n",
    "    if binary_p < 0.001:\n",
    "        binary_sig = \"*** (highly significant)\"\n",
    "    elif binary_p < 0.01:\n",
    "        binary_sig = \"** (significant)\"\n",
    "    elif binary_p < 0.05:\n",
    "        binary_sig = \"* (marginally significant)\"\n",
    "    else:\n",
    "        binary_sig = \"(not significant)\"\n",
    "    \n",
    "    print(f\"      Significance: {binary_sig}\")\n",
    "    \n",
    "    print(f\"    Graduated correlation (length vs quality score):\")\n",
    "    print(f\"      Correlation: {graduated_corr:6.3f}\")\n",
    "    print(f\"      P-value: {graduated_p:8.4f}\")\n",
    "    \n",
    "    if graduated_p < 0.001:\n",
    "        graduated_sig = \"*** (highly significant)\"\n",
    "    elif graduated_p < 0.01:\n",
    "        graduated_sig = \"** (significant)\"\n",
    "    elif graduated_p < 0.05:\n",
    "        graduated_sig = \"* (marginally significant)\"\n",
    "    else:\n",
    "        graduated_sig = \"(not significant)\"\n",
    "    \n",
    "    print(f\"      Significance: {graduated_sig}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    if abs(binary_corr) > 0.3:\n",
    "        strength = \"strong\"\n",
    "    elif abs(binary_corr) > 0.1:\n",
    "        strength = \"moderate\"\n",
    "    else:\n",
    "        strength = \"weak\"\n",
    "    \n",
    "    direction = \"positive\" if binary_corr > 0 else \"negative\"\n",
    "    print(f\"    Interpretation: {strength} {direction} relationship\")\n",
    "    \n",
    "    if binary_corr > 0 and binary_p < 0.05:\n",
    "        print(f\"    ðŸ“ Longer responses tend to have MORE hallucinations\")\n",
    "    elif binary_corr < 0 and binary_p < 0.05:\n",
    "        print(f\"    ðŸ“ Longer responses tend to have FEWER hallucinations\")\n",
    "    else:\n",
    "        print(f\"    ðŸ“ Response length doesn't significantly predict hallucinations\")\n",
    "    \n",
    "    length_accuracy_results[model_name] = {\n",
    "        'binary_correlation': binary_corr,\n",
    "        'binary_p_value': binary_p,\n",
    "        'binary_significance': binary_sig,\n",
    "        'graduated_correlation': graduated_corr,\n",
    "        'graduated_p_value': graduated_p,\n",
    "        'graduated_significance': graduated_sig,\n",
    "        'strength': strength,\n",
    "        'direction': direction\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4187f6-3803-4e93-a34f-1e3b60ee1f86",
   "metadata": {},
   "source": [
    "#### Domain differences in response verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "912a25c0-1509-425d-9b5e-605161d8c9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ðŸ“Š Claude_3.5_Sonnet:\n",
      "    General Knowledge: mean= 316.0, median= 313.5, std= 131.4\n",
      "    Science        : mean= 426.3, median= 377.0, std= 206.7\n",
      "    History        : mean= 454.3, median= 414.5, std= 207.6\n",
      "    Pop Culture    : mean= 248.4, median= 229.5, std= 130.8\n",
      "    Healthcare     : mean= 513.0, median= 481.0, std= 186.5\n",
      "    ðŸ—£ï¸  Most verbose: Healthcare (513.0 chars)\n",
      "    ðŸ¤ Least verbose: Pop Culture (248.4 chars)\n",
      "\n",
      "  ðŸ“Š GPT_3.5:\n",
      "    General Knowledge: mean=  59.5, median=  48.0, std=  53.4\n",
      "    Science        : mean= 107.7, median=  88.0, std=  81.1\n",
      "    History        : mean= 115.0, median=  84.0, std=  83.1\n",
      "    Pop Culture    : mean=  86.1, median=  77.5, std=  65.3\n",
      "    Healthcare     : mean= 123.8, median= 105.0, std=  91.6\n",
      "    ðŸ—£ï¸  Most verbose: Healthcare (123.8 chars)\n",
      "    ðŸ¤ Least verbose: General Knowledge (59.5 chars)\n",
      "\n",
      "  ðŸ“Š GPT_4o:\n",
      "    General Knowledge: mean= 149.4, median= 106.5, std= 121.6\n",
      "    Science        : mean= 326.5, median= 296.5, std= 266.8\n",
      "    History        : mean= 237.2, median= 206.5, std= 150.4\n",
      "    Pop Culture    : mean= 158.2, median= 131.0, std=  87.7\n",
      "    Healthcare     : mean= 397.1, median= 372.5, std= 215.6\n",
      "    ðŸ—£ï¸  Most verbose: Healthcare (397.1 chars)\n",
      "    ðŸ¤ Least verbose: General Knowledge (149.4 chars)\n"
     ]
    }
   ],
   "source": [
    "domain_verbosity = {}\n",
    "    \n",
    "for model_name, df in datasets.items():\n",
    "    print(f\"\\n  ðŸ“Š {model_name}:\")\n",
    "    \n",
    "    model_domain_stats = {}\n",
    "    \n",
    "    for domain in df['domain'].unique():\n",
    "        domain_data = df[df['domain'] == domain]\n",
    "        \n",
    "        domain_stats = {\n",
    "            'mean_length': domain_data['response_length'].mean(),\n",
    "            'median_length': domain_data['response_length'].median(),\n",
    "            'std_length': domain_data['response_length'].std(),\n",
    "            'count': len(domain_data)\n",
    "        }\n",
    "        \n",
    "        print(f\"    {domain:15}: mean={domain_stats['mean_length']:6.1f}, \"\n",
    "              f\"median={domain_stats['median_length']:6.1f}, \"\n",
    "              f\"std={domain_stats['std_length']:6.1f}\")\n",
    "        \n",
    "        model_domain_stats[domain] = domain_stats\n",
    "    \n",
    "    # Identify most and least verbose domains\n",
    "    domain_means = {domain: stats['mean_length'] \n",
    "                   for domain, stats in model_domain_stats.items()}\n",
    "    \n",
    "    most_verbose = max(domain_means, key=domain_means.get)\n",
    "    least_verbose = min(domain_means, key=domain_means.get)\n",
    "    \n",
    "    print(f\"    ðŸ—£ï¸  Most verbose: {most_verbose} ({domain_means[most_verbose]:.1f} chars)\")\n",
    "    print(f\"    ðŸ¤ Least verbose: {least_verbose} ({domain_means[least_verbose]:.1f} chars)\")\n",
    "    \n",
    "    domain_verbosity[model_name] = {\n",
    "        'domain_stats': model_domain_stats,\n",
    "        'most_verbose': most_verbose,\n",
    "        'least_verbose': least_verbose,\n",
    "        'verbosity_range': domain_means[most_verbose] - domain_means[least_verbose]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c78346-8554-4396-bdc6-2a87063f5c72",
   "metadata": {},
   "source": [
    "## **Simple Summary:**\n",
    "\n",
    "### **ðŸ“Š Model Verbosity Ranking:**\n",
    "1. **Claude**: Most verbose (316-513 chars average)\n",
    "2. **GPT-4o**: Medium verbose (149-397 chars average)  \n",
    "3. **GPT-3.5**: Least verbose (60-124 chars average)\n",
    "\n",
    "### **ðŸ¥ Domain Pattern (All Models):**\n",
    "- **Most verbose domain**: Healthcare\n",
    "- **Least verbose domain**: General Knowledge or Pop Culture\n",
    "\n",
    "### **ðŸŽ¯ Key Insights:**\n",
    "\n",
    "**Claude gives long, detailed responses** (2-4x longer than others)\n",
    "\n",
    "**GPT-3.5 gives very short responses** (often just 1-2 sentences)\n",
    "\n",
    "**GPT-4o is in the middle** (moderate length responses)\n",
    "\n",
    "**Healthcare questions make all models verbose** - probably because medical topics require detailed explanations\n",
    "\n",
    "**Pop Culture and General Knowledge get shorter answers** - probably because they're simpler factual questions\n",
    "\n",
    "### **ðŸ“ Bottom Line:**\n",
    "If you want **detailed explanations** â†’ Choose Claude\n",
    "If you want **concise answers** â†’ Choose GPT-3.5  \n",
    "If you want **balanced responses** â†’ Choose GPT-4o\n",
    "\n",
    "Healthcare consistently gets the longest responses regardless of model, suggesting domain complexity drives verbosity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766757a7-e336-481a-811e-d1992e818fb4",
   "metadata": {},
   "source": [
    "#### Length categories and hallucination analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52ca7dcd-fa98-4ab2-8493-647329c8c3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ðŸ“Š Claude_3.5_Sonnet:\n",
      "    Short       : 17/100 ( 17.0%) [6-250 chars]\n",
      "    Medium-Short: 10/101 (  9.9%) [251-349 chars]\n",
      "    Medium-Long : 16/ 99 ( 16.2%) [350-501 chars]\n",
      "    Long        :  6/100 (  6.0%) [504-1075 chars]\n",
      "    Trend analysis:\n",
      "      Correlation: -0.659\n",
      "      P-value:   0.3415\n",
      "      âœ— No significant trend\n",
      "\n",
      "  ðŸ“Š GPT_3.5:\n",
      "    Short       : 22/102 ( 21.6%) [4-47 chars]\n",
      "    Medium-Short: 18/ 99 ( 18.2%) [49-83 chars]\n",
      "    Medium-Long : 22/ 99 ( 22.2%) [84-126 chars]\n",
      "    Long        : 13/100 ( 13.0%) [127-416 chars]\n",
      "    Trend analysis:\n",
      "      Correlation: -0.663\n",
      "      P-value:   0.3370\n",
      "      âœ— No significant trend\n",
      "\n",
      "  ðŸ“Š GPT_4o:\n",
      "    Short       :  9/102 (  8.8%) [8-101 chars]\n",
      "    Medium-Short: 18/ 98 ( 18.4%) [102-207 chars]\n",
      "    Medium-Long : 15/100 ( 15.0%) [208-349 chars]\n",
      "    Long        :  5/100 (  5.0%) [352-1487 chars]\n",
      "    Trend analysis:\n",
      "      Correlation: -0.319\n",
      "      P-value:   0.6814\n",
      "      âœ— No significant trend\n"
     ]
    }
   ],
   "source": [
    "length_category_results = {}\n",
    "\n",
    "for model_name, df in datasets.items():\n",
    "    print(f\"\\n  ðŸ“Š {model_name}:\")\n",
    "    \n",
    "    # Define length categories based on quartiles\n",
    "    q1 = df['response_length'].quantile(0.25)\n",
    "    q2 = df['response_length'].quantile(0.50)\n",
    "    q3 = df['response_length'].quantile(0.75)\n",
    "    \n",
    "    # Create categories\n",
    "    df_cat = df.copy()\n",
    "    df_cat['length_category'] = pd.cut(df_cat['response_length'], \n",
    "                                      bins=[0, q1, q2, q3, float('inf')],\n",
    "                                      labels=['Short', 'Medium-Short', 'Medium-Long', 'Long'],\n",
    "                                      include_lowest=True)\n",
    "    \n",
    "    category_stats = {}\n",
    "    \n",
    "    for category in ['Short', 'Medium-Short', 'Medium-Long', 'Long']:\n",
    "        cat_data = df_cat[df_cat['length_category'] == category]\n",
    "        \n",
    "        if len(cat_data) > 0:\n",
    "            total = len(cat_data)\n",
    "            hallucinated = cat_data['hallucination_present'].sum()\n",
    "            rate = (hallucinated / total) * 100\n",
    "            \n",
    "            category_stats[category] = {\n",
    "                'total': total,\n",
    "                'hallucinated': hallucinated,\n",
    "                'rate': rate,\n",
    "                'length_range': f\"{cat_data['response_length'].min():.0f}-{cat_data['response_length'].max():.0f}\"\n",
    "            }\n",
    "            \n",
    "            print(f\"    {category:12}: {hallucinated:2d}/{total:3d} ({rate:5.1f}%) \"\n",
    "                  f\"[{category_stats[category]['length_range']} chars]\")\n",
    "    \n",
    "    # Test for trend\n",
    "    if len(category_stats) >= 3:\n",
    "        rates = [stats['rate'] for stats in category_stats.values()]\n",
    "        categories_numeric = list(range(len(rates)))\n",
    "        \n",
    "        trend_corr, trend_p = pearsonr(categories_numeric, rates)\n",
    "        \n",
    "        print(f\"    Trend analysis:\")\n",
    "        print(f\"      Correlation: {trend_corr:6.3f}\")\n",
    "        print(f\"      P-value: {trend_p:8.4f}\")\n",
    "        \n",
    "        if trend_p < 0.05:\n",
    "            if trend_corr > 0:\n",
    "                print(f\"      âœ“ Significant upward trend (longer â†’ more hallucinations)\")\n",
    "            else:\n",
    "                print(f\"      âœ“ Significant downward trend (longer â†’ fewer hallucinations)\")\n",
    "        else:\n",
    "            print(f\"      âœ— No significant trend\")\n",
    "    \n",
    "    length_category_results[model_name] = category_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04fff1f-fc10-47e3-b942-50198899b6c9",
   "metadata": {},
   "source": [
    "## **Simple Summary:**\n",
    "\n",
    "### **ðŸŽ¯ Key Finding: Longer Responses Are Actually MORE Accurate**\n",
    "\n",
    "All three models show the **same pattern**:\n",
    "- **Short responses**: Highest hallucination rates (17-22%)\n",
    "- **Long responses**: Lowest hallucination rates (5-13%)\n",
    "\n",
    "### **ðŸ“Š Model Performance by Length:**\n",
    "\n",
    "**Claude:**\n",
    "- Short (6-250 chars): 17% hallucinations âŒ\n",
    "- Long (504+ chars): 6% hallucinations âœ…\n",
    "\n",
    "**GPT-3.5:** \n",
    "- Short (4-47 chars): 22% hallucinations âŒ\n",
    "- Long (127+ chars): 13% hallucinations âœ…\n",
    "\n",
    "**GPT-4o:**\n",
    "- Short (8-101 chars): 9% hallucinations âŒ  \n",
    "- Long (352+ chars): 5% hallucinations âœ…\n",
    "\n",
    "### **ðŸ” What This Means:**\n",
    "\n",
    "**When models give short answers, they're more likely to be wrong**\n",
    "- Maybe they're being brief because they're uncertain\n",
    "- Maybe they're oversimplifying complex topics\n",
    "\n",
    "**When models give long answers, they're more likely to be accurate**\n",
    "- More detail = more opportunity to get it right\n",
    "- Longer responses might indicate confidence\n",
    "\n",
    "### **âš ï¸ Important Note:**\n",
    "The trend isn't statistically significant (p > 0.05), so we can't be 100% certain this pattern is real.\n",
    "\n",
    "### **ðŸ“ Bottom Line:**\n",
    "**Suspicious short answers!** When any of these models gives you a very brief response, be extra cautious - it's more likely to contain errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8494027-6adf-4284-b753-c7ec189e323f",
   "metadata": {},
   "source": [
    "#### Citation usage rates by model and domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41d949d9-ac96-4cbe-b27e-5dd75dbd85d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ðŸ“Š Claude_3.5_Sonnet:\n",
      "    Overall citation rate:  26/400 (  6.5%)\n",
      "    By domain:\n",
      "      General Knowledge:  3/80 (  3.8%)\n",
      "      Science        :  1/80 (  1.2%)\n",
      "      History        : 13/80 ( 16.2%)\n",
      "      Pop Culture    :  4/80 (  5.0%)\n",
      "      Healthcare     :  5/80 (  6.2%)\n",
      "    ðŸ“ˆ Highest citation domain: History (16.2%)\n",
      "    ðŸ“‰ Lowest citation domain: Science (1.2%)\n",
      "\n",
      "  ðŸ“Š GPT_3.5:\n",
      "    Overall citation rate:   1/400 (  0.2%)\n",
      "    By domain:\n",
      "      General Knowledge:  0/80 (  0.0%)\n",
      "      Science        :  0/80 (  0.0%)\n",
      "      History        :  1/80 (  1.2%)\n",
      "      Pop Culture    :  0/80 (  0.0%)\n",
      "      Healthcare     :  0/80 (  0.0%)\n",
      "    ðŸ“ˆ Highest citation domain: History (1.2%)\n",
      "    ðŸ“‰ Lowest citation domain: General Knowledge (0.0%)\n",
      "\n",
      "  ðŸ“Š GPT_4o:\n",
      "    Overall citation rate:  14/400 (  3.5%)\n",
      "    By domain:\n",
      "      General Knowledge:  2/80 (  2.5%)\n",
      "      Science        :  1/80 (  1.2%)\n",
      "      History        :  5/80 (  6.2%)\n",
      "      Pop Culture    :  1/80 (  1.2%)\n",
      "      Healthcare     :  5/80 (  6.2%)\n",
      "    ðŸ“ˆ Highest citation domain: History (6.2%)\n",
      "    ðŸ“‰ Lowest citation domain: Science (1.2%)\n"
     ]
    }
   ],
   "source": [
    "citation_usage = {}\n",
    "    \n",
    "for model_name, df in datasets.items():\n",
    "    print(f\"\\n  ðŸ“Š {model_name}:\")\n",
    "    \n",
    "    # Overall citation rate\n",
    "    total_responses = len(df)\n",
    "    citations_used = df['citation_present'].sum()\n",
    "    overall_rate = (citations_used / total_responses) * 100\n",
    "    \n",
    "    print(f\"    Overall citation rate: {citations_used:3d}/{total_responses} ({overall_rate:5.1f}%)\")\n",
    "    \n",
    "    # By domain\n",
    "    domain_citation_stats = {}\n",
    "    print(f\"    By domain:\")\n",
    "    \n",
    "    for domain in df['domain'].unique():\n",
    "        domain_data = df[df['domain'] == domain]\n",
    "        domain_total = len(domain_data)\n",
    "        domain_citations = domain_data['citation_present'].sum()\n",
    "        domain_rate = (domain_citations / domain_total) * 100 if domain_total > 0 else 0\n",
    "        \n",
    "        domain_citation_stats[domain] = {\n",
    "            'total': domain_total,\n",
    "            'citations': domain_citations,\n",
    "            'rate': domain_rate\n",
    "        }\n",
    "        \n",
    "        print(f\"      {domain:15}: {domain_citations:2d}/{domain_total:2d} ({domain_rate:5.1f}%)\")\n",
    "    \n",
    "    # Find highest/lowest citation domains\n",
    "    if domain_citation_stats:\n",
    "        highest_cite_domain = max(domain_citation_stats.items(), key=lambda x: x[1]['rate'])\n",
    "        lowest_cite_domain = min(domain_citation_stats.items(), key=lambda x: x[1]['rate'])\n",
    "        \n",
    "        print(f\"    ðŸ“ˆ Highest citation domain: {highest_cite_domain[0]} ({highest_cite_domain[1]['rate']:.1f}%)\")\n",
    "        print(f\"    ðŸ“‰ Lowest citation domain: {lowest_cite_domain[0]} ({lowest_cite_domain[1]['rate']:.1f}%)\")\n",
    "    \n",
    "    citation_usage[model_name] = {\n",
    "        'overall_rate': overall_rate,\n",
    "        'total_citations': citations_used,\n",
    "        'domain_stats': domain_citation_stats,\n",
    "        'highest_cite_domain': highest_cite_domain[0] if domain_citation_stats else None,\n",
    "        'lowest_cite_domain': lowest_cite_domain[0] if domain_citation_stats else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701450ea-bf24-4377-ae8c-9df31ea689f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
