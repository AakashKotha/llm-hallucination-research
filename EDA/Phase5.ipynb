{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "527db3a2-1f49-464e-8174-3f80906aa150",
   "metadata": {},
   "source": [
    "### Phase 5: Model Comparison Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23c9a35f-98d6-4d6e-aa75-0644911515bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr, chi2_contingency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09c5eb9b-08e1-4a65-8c2f-9051a6511409",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "model_names = ['Claude_3.5_Sonnet', 'GPT_3.5', 'GPT_4o']\n",
    "file_paths = [\n",
    "    '../Data/qna_dataset_Claude3.5Sonnet_final.csv',\n",
    "    '../Data/qna_dataset_GPT3.5_final.csv', \n",
    "    '../Data/qna_dataset_GPT4o_final.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b606080f-6b93-42f8-921d-2f50c2a7a726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Claude_3.5_Sonnet: 400 rows Ã— 16 columns\n",
      "âœ“ GPT_3.5: 400 rows Ã— 16 columns\n",
      "âœ“ GPT_4o: 400 rows Ã— 16 columns\n"
     ]
    }
   ],
   "source": [
    "for model_name, file_path in zip(model_names, file_paths):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        datasets[model_name] = df\n",
    "        print(f\"âœ“ {model_name}: {df.shape[0]} rows Ã— {df.shape[1]} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1b1ce0c-3c5f-4125-ad29-3b4ea7f9a400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Analysis Setup:\n",
      "  Common questions across all models: 400\n",
      "  Models being compared: ['Claude_3.5_Sonnet', 'GPT_3.5', 'GPT_4o']\n"
     ]
    }
   ],
   "source": [
    "model_names = list(datasets.keys())\n",
    "question_ids = {}\n",
    "\n",
    "for model_name, df in datasets.items():\n",
    "    question_ids[model_name] = set(df['question_id'].unique())\n",
    "\n",
    "# Find common questions\n",
    "common_questions = set.intersection(*question_ids.values())\n",
    "print(f\"ðŸ“Š Analysis Setup:\")\n",
    "print(f\"  Common questions across all models: {len(common_questions)}\")\n",
    "print(f\"  Models being compared: {model_names}\")\n",
    "\n",
    "if len(common_questions) == 0:\n",
    "    print(\"âŒ No common questions found - cannot perform head-to-head analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da4a82-2481-4956-a55d-07a75ec946be",
   "metadata": {},
   "source": [
    "#### Same question, different models - find disagreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d0fd869-c592-484d-884b-c2fc13cc32bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ“Š Agreement Statistics:\n",
      "    Perfect agreement: 320/400 ( 80.0%)\n",
      "    Partial disagreement:  80/400 ( 20.0%)\n",
      "    Complete disagreement:   0/400 (  0.0%)\n"
     ]
    }
   ],
   "source": [
    "disagreement_analysis = {}\n",
    "agreement_patterns = {}\n",
    "\n",
    "# Create combined dataset for analysis\n",
    "combined_results = []\n",
    "\n",
    "for question_id in sorted(common_questions):\n",
    "    question_results = {'question_id': question_id}\n",
    "    \n",
    "    # Get question text and metadata (from first model)\n",
    "    first_model = model_names[0]\n",
    "    question_row = datasets[first_model][datasets[first_model]['question_id'] == question_id].iloc[0]\n",
    "    question_results.update({\n",
    "        'question_text': question_row['question_text'],\n",
    "        'domain': question_row['domain'],\n",
    "        'question_length': question_row['question_length'],\n",
    "        'question_type': question_row['question_type'],\n",
    "        'question_style': question_row['question_style']\n",
    "    })\n",
    "    \n",
    "    # Get results from each model\n",
    "    model_results = {}\n",
    "    for model_name in model_names:\n",
    "        model_row = datasets[model_name][datasets[model_name]['question_id'] == question_id].iloc[0]\n",
    "        model_results[model_name] = {\n",
    "            'hallucination': model_row['hallucination_present'],\n",
    "            'factscore': model_row['factscore'],\n",
    "            'response_length': model_row['response_length'],\n",
    "            'citation_present': model_row.get('citation_present', False)\n",
    "        }\n",
    "    \n",
    "    question_results['model_results'] = model_results\n",
    "    combined_results.append(question_results)\n",
    "\n",
    "# Analyze agreement patterns\n",
    "total_questions = len(combined_results)\n",
    "\n",
    "# Perfect agreement (all models agree)\n",
    "perfect_agreement = 0\n",
    "complete_disagreement = 0\n",
    "partial_disagreement = 0\n",
    "\n",
    "disagreement_details = []\n",
    "\n",
    "for result in combined_results:\n",
    "    halluc_results = [result['model_results'][model]['hallucination'] for model in model_names]\n",
    "    unique_results = set(halluc_results)\n",
    "    \n",
    "    if len(unique_results) == 1:\n",
    "        perfect_agreement += 1\n",
    "    elif len(unique_results) == len(model_names):\n",
    "        complete_disagreement += 1\n",
    "        disagreement_details.append({\n",
    "            'question_id': result['question_id'],\n",
    "            'domain': result['domain'],\n",
    "            'question_text': result['question_text'][:100] + \"...\",\n",
    "            'type': 'complete',\n",
    "            'model_results': result['model_results']\n",
    "        })\n",
    "    else:\n",
    "        partial_disagreement += 1\n",
    "        disagreement_details.append({\n",
    "            'question_id': result['question_id'],\n",
    "            'domain': result['domain'],\n",
    "            'question_text': result['question_text'][:100] + \"...\",\n",
    "            'type': 'partial',\n",
    "            'model_results': result['model_results']\n",
    "        })\n",
    "\n",
    "print(f\"  ðŸ“Š Agreement Statistics:\")\n",
    "print(f\"    Perfect agreement: {perfect_agreement:3d}/{total_questions} ({perfect_agreement/total_questions*100:5.1f}%)\")\n",
    "print(f\"    Partial disagreement: {partial_disagreement:3d}/{total_questions} ({partial_disagreement/total_questions*100:5.1f}%)\")\n",
    "print(f\"    Complete disagreement: {complete_disagreement:3d}/{total_questions} ({complete_disagreement/total_questions*100:5.1f}%)\")\n",
    "\n",
    "agreement_patterns = {\n",
    "    'perfect_agreement': perfect_agreement,\n",
    "    'partial_disagreement': partial_disagreement,\n",
    "    'complete_disagreement': complete_disagreement,\n",
    "    'total_questions': total_questions\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0a3ee9e-1968-41db-adb1-e5cccba769e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% Perfect Agreement = All 3 models gave the same result (all correct OR all wrong) on 320 questions\n",
    "# 20% Partial Disagreement = On 80 questions, some models were right while others were wrong\n",
    "# 0% Complete Disagreement = Never had all 3 models disagree with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3c980de-5a02-4d7e-b127-1813cfd21ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‹ Examples of Model Disagreements:\n",
      "-------------------------------------------------------\n",
      "\n",
      "  Example 1 (partial disagreement):\n",
      "    Question ID: 1\n",
      "    Domain: General Knowledge\n",
      "    Question: What is the most common bird in the world?...\n",
      "    Model results:\n",
      "      Claude_3.5_Sonnet: âŒ somewhat inaccurate\n",
      "      GPT_3.5        : âœ… completely right\n",
      "      GPT_4o         : âœ… completely right\n",
      "\n",
      "  Example 2 (partial disagreement):\n",
      "    Question ID: 4\n",
      "    Domain: General Knowledge\n",
      "    Question: \"In a 2007 interview, which actor 'animatedly' bemoaned \"\"I hate that cat! Ever since I did that cat...\n",
      "    Model results:\n",
      "      Claude_3.5_Sonnet: âœ… somewhat correct\n",
      "      GPT_3.5        : âŒ totally wrong\n",
      "      GPT_4o         : âœ… completely right\n",
      "\n",
      "  Example 3 (partial disagreement):\n",
      "    Question ID: 10\n",
      "    Domain: General Knowledge\n",
      "    Question: Which role is being played in a recently released film by the actor whose previous roles include Tim...\n",
      "    Model results:\n",
      "      Claude_3.5_Sonnet: âœ… somewhat correct\n",
      "      GPT_3.5        : âŒ totally wrong\n",
      "      GPT_4o         : âœ… somewhat inaccurate\n",
      "\n",
      "  Example 4 (partial disagreement):\n",
      "    Question ID: 17\n",
      "    Domain: General Knowledge\n",
      "    Question: What word is used in betting slang for odds of five to one (5/1)?...\n",
      "    Model results:\n",
      "      Claude_3.5_Sonnet: âœ… somewhat correct\n",
      "      GPT_3.5        : âœ… somewhat correct\n",
      "      GPT_4o         : âŒ somewhat inaccurate\n",
      "\n",
      "  Example 5 (partial disagreement):\n",
      "    Question ID: 23\n",
      "    Domain: General Knowledge\n",
      "    Question: The youngest ever member of the Order of Merit was admitted in 1968 and is still a member. Who is he...\n",
      "    Model results:\n",
      "      Claude_3.5_Sonnet: âŒ somewhat inaccurate\n",
      "      GPT_3.5        : âœ… somewhat correct\n",
      "      GPT_4o         : âŒ somewhat inaccurate\n"
     ]
    }
   ],
   "source": [
    " print(f\"\\nðŸ“‹ Examples of Model Disagreements:\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "# Show top 5 disagreements\n",
    "disagreement_examples = disagreement_details[:5]\n",
    "\n",
    "for i, example in enumerate(disagreement_examples, 1):\n",
    "    print(f\"\\n  Example {i} ({example['type']} disagreement):\")\n",
    "    print(f\"    Question ID: {example['question_id']}\")\n",
    "    print(f\"    Domain: {example['domain']}\")\n",
    "    print(f\"    Question: {example['question_text']}\")\n",
    "    print(f\"    Model results:\")\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        halluc = example['model_results'][model_name]['hallucination']\n",
    "        factscore = example['model_results'][model_name]['factscore']\n",
    "        halluc_symbol = \"âŒ\" if halluc else \"âœ…\"\n",
    "        print(f\"      {model_name:15}: {halluc_symbol} {factscore}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306cd73a-97d1-4702-83df-b04e216faef2",
   "metadata": {},
   "source": [
    "#### Questions where all models struggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "296779f8-089d-4d41-8987-3c64fd68b57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ“Š Universal Failure Statistics:\n",
      "    Questions where all models failed: 21/400 (5.2%)\n",
      "\n",
      "  ðŸ” Characteristics of Difficult Questions:\n",
      "    Average length: 78.7 characters\n",
      "    Domain distribution:\n",
      "      Pop Culture    : 10 ( 47.6%)\n",
      "      History        :  6 ( 28.6%)\n",
      "      General Knowledge:  4 ( 19.0%)\n",
      "      Healthcare     :  1 (  4.8%)\n",
      "    Type distribution:\n",
      "      closed-ended   : 21 (100.0%)\n",
      "\n",
      "  ðŸ“‹ Examples of Universally Difficult Questions:\n",
      "    1. [General Knowledge] Who was the first British winner of the US Womenâ€™s Open?...\n",
      "    2. [General Knowledge] What was the (2011 reported) average annual salary of a UK ('county') Council Chief Executive?...\n",
      "    3. [General Knowledge] Elected in 2008, who is the current Prime Minister of New Zealand?...\n"
     ]
    }
   ],
   "source": [
    "difficult_questions = []\n",
    "\n",
    "for result in combined_results:\n",
    "    # Check if all models hallucinated\n",
    "    all_hallucinated = all(result['model_results'][model]['hallucination'] \n",
    "                          for model in model_names)\n",
    "    \n",
    "    if all_hallucinated:\n",
    "        difficult_questions.append({\n",
    "            'question_id': result['question_id'],\n",
    "            'domain': result['domain'],\n",
    "            'question_text': result['question_text'],\n",
    "            'question_length': result['question_length'],\n",
    "            'question_type': result['question_type']\n",
    "        })\n",
    "\n",
    "print(f\"  ðŸ“Š Universal Failure Statistics:\")\n",
    "print(f\"    Questions where all models failed: {len(difficult_questions)}/{total_questions} ({len(difficult_questions)/total_questions*100:.1f}%)\")\n",
    "\n",
    "if difficult_questions:\n",
    "    print(f\"\\n  ðŸ” Characteristics of Difficult Questions:\")\n",
    "    \n",
    "    # Analyze patterns in difficult questions\n",
    "    difficult_domains = Counter(q['domain'] for q in difficult_questions)\n",
    "    difficult_types = Counter(q['question_type'] for q in difficult_questions)\n",
    "    \n",
    "    avg_length = np.mean([q['question_length'] for q in difficult_questions])\n",
    "    \n",
    "    print(f\"    Average length: {avg_length:.1f} characters\")\n",
    "    print(f\"    Domain distribution:\")\n",
    "    for domain, count in difficult_domains.most_common():\n",
    "        pct = (count / len(difficult_questions)) * 100\n",
    "        print(f\"      {domain:15}: {count:2d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    print(f\"    Type distribution:\")\n",
    "    for qtype, count in difficult_types.most_common():\n",
    "        pct = (count / len(difficult_questions)) * 100\n",
    "        print(f\"      {qtype:15}: {count:2d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Show examples\n",
    "    print(f\"\\n  ðŸ“‹ Examples of Universally Difficult Questions:\")\n",
    "    for i, q in enumerate(difficult_questions[:3], 1):\n",
    "        print(f\"    {i}. [{q['domain']}] {q['question_text']}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a48136-760d-40fc-8cd5-22279a1b54fc",
   "metadata": {},
   "source": [
    "#### Calculate how often each model agrees with the majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecd3eef6-b2ca-46d2-8eb6-3d2c94cf1575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ“Š Consistency with Majority Decision:\n",
      "    ðŸ¥‡ 1. GPT_4o         :  95.5% (Most consistent)\n",
      "       2. Claude_3.5_Sonnet:  93.0%\n",
      "    ðŸ¥‰ 3. GPT_3.5        :  91.5% (Least consistent)\n"
     ]
    }
   ],
   "source": [
    "consistency_scores = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    agreements = 0\n",
    "    \n",
    "    for result in combined_results:\n",
    "        # Get majority decision\n",
    "        halluc_votes = [result['model_results'][m]['hallucination'] for m in model_names]\n",
    "        majority_halluc = sum(halluc_votes) > len(model_names) / 2\n",
    "        \n",
    "        # Check if this model agrees with majority\n",
    "        model_halluc = result['model_results'][model_name]['hallucination']\n",
    "        if model_halluc == majority_halluc:\n",
    "            agreements += 1\n",
    "    \n",
    "    consistency_score = (agreements / total_questions) * 100\n",
    "    consistency_scores[model_name] = consistency_score\n",
    "\n",
    "# Rank by consistency\n",
    "ranked_consistency = sorted(consistency_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"  ðŸ“Š Consistency with Majority Decision:\")\n",
    "for i, (model, score) in enumerate(ranked_consistency, 1):\n",
    "    if i == 1:\n",
    "        print(f\"    ðŸ¥‡ {i}. {model:15}: {score:5.1f}% (Most consistent)\")\n",
    "    elif i == len(ranked_consistency):\n",
    "        print(f\"    ðŸ¥‰ {i}. {model:15}: {score:5.1f}% (Least consistent)\")\n",
    "    else:\n",
    "        print(f\"       {i}. {model:15}: {score:5.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fe2a0f-2319-4baf-826b-aa013199e061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
